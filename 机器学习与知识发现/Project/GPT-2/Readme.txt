cache文件夹内是不同的vocab词表

config是GPT-2模型不同大小的设置

data内保存了训练所用的.json源数据 tokenized则是用Bert的tokenizer分词并转化为id后得到的最终直接用于训练的格式
这个格式由build_tokenized.py得到

data_preprocessing是一些数据预处理的脚本

model内保存了训练后得到的模型，由于大小较大，我们只放了一个贴吧的模型供助教测(wan)试(shua)

train_mine.py用于训练 如果有apex的话可以使用混合精度加速 stride这个参数是在整个序列中一个batch移动的步数 
warm_up steps用于schedule学习率，在训练过程中发现延长这个过程最后可以训练得到更小的loss

generate_mine用于在命令行中进行生成，要指定platform（python generate_mine.py tieba) 可以调试不同的top_k, top_p, temperature,
repetition_penalty等参数进行生成

interaction是我们做的一个比较简陋的UI

GPT2的中文资源非常少，我们的工作很多地方参考了https://github.com/Morizeyao/GPT2-Chinese 也对数据读取等一些地方做了优化